{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc22ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFA chargé\n",
      "DFB chargé\n",
      "DFC chargé\n",
      "DFD chargé\n",
      "DFE chargé\n",
      "DFF chargé\n",
      "DFH chargé\n",
      "DFI chargé\n",
      "DFJ chargé\n",
      "DFK chargé\n",
      "DFL chargé\n",
      "DFM chargé\n",
      "DFN chargé\n",
      "DFO chargé\n",
      "DFP chargé\n",
      "DFQ chargé\n",
      "DFR chargé\n",
      "DFS chargé\n",
      "DFT chargé\n",
      "DFV chargé\n",
      "DFW chargé\n",
      "DFX chargé\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: EOF inside string starting at row 1972",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 305\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename_Y, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    304\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(response\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m--> 305\u001b[0m DFY \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(filename_Y, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDFY chargé\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    307\u001b[0m save_to_db(DFY, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdfy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1697\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1699\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1700\u001b[0m     (\n\u001b[0;32m   1701\u001b[0m         index,\n\u001b[0;32m   1702\u001b[0m         columns,\n\u001b[0;32m   1703\u001b[0m         col_dict,\n\u001b[1;32m-> 1704\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m         nrows\n\u001b[0;32m   1706\u001b[0m     )\n\u001b[0;32m   1707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:814\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:875\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:850\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:2029\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at row 1972"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Connexion à la base de données\n",
    "engine = create_engine(\"postgresql+psycopg2://postgres:Amyas01062015%40@localhost:5432/olist_ods\")\n",
    "schema = 'ODS_Projet_etudes'\n",
    "\n",
    "def save_to_db(df, table_name):\n",
    "    df.to_sql(table_name, engine, schema=schema, if_exists='replace', index=False)\n",
    "\n",
    "# A\n",
    "url_A = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/metro_localisation_stations@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_A = \"Métro du réseau STAR  localisation des stations.csv\"\n",
    "response = requests.get(url_A)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_A, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFA = pd.read_csv(filename_A, delimiter=';')\n",
    "    print(\"DFA chargé\")\n",
    "    save_to_db(DFA, 'dfa')\n",
    "else:\n",
    "    print(\"Erreur pour A\")\n",
    "\n",
    "# B\n",
    "url_B = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/tco-bus-circulation-passages-tr@keolis-rennes/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_B = \"Prochains passages des lignes de bus du réseau STAR en temps réel.csv\"\n",
    "response = requests.get(url_B)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_B, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFB = pd.read_csv(filename_B, delimiter=';')\n",
    "    print(\"DFB chargé\")\n",
    "    save_to_db(DFB, 'dfb')\n",
    "else:\n",
    "    print(\"Erreur pour B\")\n",
    "\n",
    "# C\n",
    "url_C = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/tco-bus-vehicules-position-tr@keolis-rennes/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_C = \"Position des bus en circulation sur le réseau STAR en temps réel.csv\"\n",
    "response = requests.get(url_C)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_C, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFC = pd.read_csv(filename_C, delimiter=';')\n",
    "    print(\"DFC chargé\")\n",
    "    save_to_db(DFC, 'dfc')\n",
    "else:\n",
    "    print(\"Erreur pour C\")\n",
    "\n",
    "# D\n",
    "url_D = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/tco-metro-circulation-passages-tr@keolis-rennes/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_D = \"Prochains passages applicables des lignes de métro du réseau STAR.csv\"\n",
    "response = requests.get(url_D)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_D, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFD = pd.read_csv(filename_D, delimiter=';')\n",
    "    print(\"DFD chargé\")\n",
    "    save_to_db(DFD, 'dfd')\n",
    "else:\n",
    "    print(\"Erreur pour D\")\n",
    "\n",
    "# E\n",
    "url_E = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/vls-stations-etat-tr@keolis-rennes/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_E = \"Etat des stations de vélos en libre-service en temps réel.csv\"\n",
    "response = requests.get(url_E)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_E, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFE = pd.read_csv(filename_E, delimiter=';')\n",
    "    print(\"DFE chargé\")\n",
    "    save_to_db(DFE, 'dfe')\n",
    "else:\n",
    "    print(\"Erreur pour E\")\n",
    "\n",
    "# F\n",
    "url_F = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/tco-metro-equipements-etat-tr@keolis-rennes/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_F = \"Etat des équipements des stations de métro du réseau STAR en temps réel.csv\"\n",
    "response = requests.get(url_F)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_F, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFF = pd.read_csv(filename_F, delimiter=';')\n",
    "    print(\"DFF chargé\")\n",
    "    save_to_db(DFF, 'dff')\n",
    "else:\n",
    "    print(\"Erreur pour F\")\n",
    "\n",
    "# H\n",
    "url_H = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/mkt-frequentation-niveau-freq-max-ligne@keolis-rennes/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_H = \"Niveau de fréquentation maximale par ligne STAR.csv\"\n",
    "response = requests.get(url_H)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_H, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFH = pd.read_csv(filename_H, delimiter=';')\n",
    "    print(\"DFH chargé\")\n",
    "    save_to_db(DFH, 'dfh')\n",
    "else:\n",
    "    print(\"Erreur pour H\")\n",
    "\n",
    "# I\n",
    "url_I = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/export-api-parking-citedia@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_I = \"Occupation et tarifs des parkings Rennes.csv\"\n",
    "response = requests.get(url_I)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_I, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFI = pd.read_csv(filename_I, delimiter=';')\n",
    "    print(\"DFI chargé\")\n",
    "    save_to_db(DFI, 'dfi')\n",
    "else:\n",
    "    print(\"Erreur pour I\")\n",
    "\n",
    "# J\n",
    "url_J = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/tco-parcsrelais-star-etat-tr@keolis-rennes/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_J = \"Etat des parcs-relais du réseau STAR en temps réel.csv\"\n",
    "response = requests.get(url_J)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_J, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFJ = pd.read_csv(filename_J, delimiter=';')\n",
    "    print(\"DFJ chargé\")\n",
    "    save_to_db(DFJ, 'dfj')\n",
    "else:\n",
    "    print(\"Erreur pour J\")\n",
    "\n",
    "# K\n",
    "url_K = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/etat-des-lignes-de-metro-du-reseau-star-en-temps-reel@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_K = \"Etat des lignes de métro du réseau STAR en temps réel.csv\"\n",
    "response = requests.get(url_K)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_K, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFK = pd.read_csv(filename_K, delimiter=';')\n",
    "    print(\"DFK chargé\")\n",
    "    save_to_db(DFK, 'dfk')\n",
    "else:\n",
    "    print(\"Erreur pour K\")\n",
    "\n",
    "# L\n",
    "url_L = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/etat-des-equipements-des-stations-de-metro-du-reseau-star-en-temps-reel@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_L = \"Etat des équipements des stations de métro du réseau STAR en temps réel.csv\"\n",
    "response = requests.get(url_L)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_L, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFL = pd.read_csv(filename_L, delimiter=';')\n",
    "    print(\"DFL chargé\")\n",
    "    save_to_db(DFL, 'dfl')\n",
    "else:\n",
    "    print(\"Erreur pour L\")\n",
    "\n",
    "# M\n",
    "url_M = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/tco-bus-topologie-parcours-td@keolis-rennes/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_M = \"Parcours des lignes de bus du réseau STAR.csv\"\n",
    "response = requests.get(url_M)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_M, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFM = pd.read_csv(filename_M, delimiter=';')\n",
    "    print(\"DFM chargé\")\n",
    "    save_to_db(DFM, 'dfm')\n",
    "else:\n",
    "    print(\"Erreur pour M\")\n",
    "\n",
    "# N\n",
    "url_N = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/tco-bus-materiel-vehicules-td@keolis-rennes/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_N = \"Bus utilisés sur le réseau STAR.csv\"\n",
    "response = requests.get(url_N)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_N, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFN = pd.read_csv(filename_N, delimiter=';')\n",
    "    print(\"DFN chargé\")\n",
    "    save_to_db(DFN, 'dfn')\n",
    "else:\n",
    "    print(\"Erreur pour N\")\n",
    "\n",
    "# O\n",
    "url_O = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/conditions-de-circulation-hivernales@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_O = \"Conditions de circulation hivernales.csv\"\n",
    "response = requests.get(url_O)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_O, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFO = pd.read_csv(filename_O, delimiter=';')\n",
    "    print(\"DFO chargé\")\n",
    "    save_to_db(DFO, 'dfo')\n",
    "else:\n",
    "    print(\"Erreur pour O\")\n",
    "\n",
    "# P\n",
    "url_P = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/tco-billettique-star-frequentation-agregee-td@keolis-rennes/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_P = \"Données de fréquentation agrégées du réseau STAR au format tableau.csv\"\n",
    "response = requests.get(url_P)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_P, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFP = pd.read_csv(filename_P, delimiter=';')\n",
    "    print(\"DFP chargé\")\n",
    "    save_to_db(DFP, 'dfp')\n",
    "else:\n",
    "    print(\"Erreur pour P\")\n",
    "\n",
    "# Q\n",
    "url_Q = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/travaux_1_jour@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_Q = \"Travaux de voirie impactant la circulation automobile sur Rennes Métropole pour le jour courant.csv\"\n",
    "response = requests.get(url_Q)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_Q, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFQ = pd.read_csv(filename_Q, delimiter=';')\n",
    "    print(\"DFQ chargé\")\n",
    "    save_to_db(DFQ, 'dfq')\n",
    "else:\n",
    "    print(\"Erreur pour Q\")\n",
    "\n",
    "# R\n",
    "url_R = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/accidents_corporels@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_R = \"Accidents corporels sur Rennes Métropole.csv\"\n",
    "response = requests.get(url_R)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_R, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFR = pd.read_csv(filename_R, delimiter=';')\n",
    "    print(\"DFR chargé\")\n",
    "    save_to_db(DFR, 'dfr')\n",
    "else:\n",
    "    print(\"Erreur pour R\")\n",
    "\n",
    "# S\n",
    "url_S = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/tco-billettique-frequentation-detaillee-td@keolis-rennes/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_S = \"Données de fréquentation détaillées du réseau STAR.csv\"\n",
    "response = requests.get(url_S)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_S, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFS = pd.read_csv(filename_S, delimiter=';')\n",
    "    print(\"DFS chargé\")\n",
    "    save_to_db(DFS, 'dfs')\n",
    "else:\n",
    "    print(\"Erreur pour S\")\n",
    "\n",
    "# T\n",
    "url_T = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/viabilite-hivernale@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_T = \"Viabilité Hivernale (VH) sur Rennes Métropole.csv\"\n",
    "response = requests.get(url_T)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_T, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFT = pd.read_csv(filename_T, delimiter=';')\n",
    "    print(\"DFT chargé\")\n",
    "    save_to_db(DFT, 'dft')\n",
    "else:\n",
    "    print(\"Erreur pour T\")\n",
    "\n",
    "# V\n",
    "url_V = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/travaux_6_jours@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_V = \"Travaux de voirie impactant la circulation automobile sur Rennes Métropole pour les 6 prochains jours.csv\"\n",
    "response = requests.get(url_V)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_V, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFV = pd.read_csv(filename_V, delimiter=';')\n",
    "    print(\"DFV chargé\")\n",
    "    save_to_db(DFV, 'dfv')\n",
    "else:\n",
    "    print(\"Erreur pour V\")\n",
    "\n",
    "# W\n",
    "url_W = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/accidents_corporels_cumul_hors_intersection@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_W = \"Cumul sur 5 ans des accidents corporels hors intersection.csv\"\n",
    "response = requests.get(url_W)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_W, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFW = pd.read_csv(filename_W, delimiter=';')\n",
    "    print(\"DFW chargé\")\n",
    "    save_to_db(DFW, 'dfw')\n",
    "else:\n",
    "    print(\"Erreur pour W\")\n",
    "\n",
    "# X\n",
    "url_X = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/intervention_securite@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_X = \"Interventions de sécurité de la Direction de la Voirie sur Rennes Métropole.csv\"\n",
    "response = requests.get(url_X)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_X, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFX = pd.read_csv(filename_X, delimiter=';')\n",
    "    print(\"DFX chargé\")\n",
    "    save_to_db(DFX, 'dfx')\n",
    "else:\n",
    "    print(\"Erreur pour X\")\n",
    "\n",
    "# Y\n",
    "url_Y = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/troncon_rva_support_fcd_avt_28032023@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_Y = \"Tronçons de voirie du RVA supports aux données de trafic routier temps réel collectées avant le 28032023.csv\"\n",
    "response = requests.get(url_Y)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_Y, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFY = pd.read_csv(filename_Y, delimiter=';')\n",
    "    print(\"DFY chargé\")\n",
    "    save_to_db(DFY, 'dfy')\n",
    "else:\n",
    "    print(\"Erreur pour Y\")\n",
    "\n",
    "# Z\n",
    "url_Z = \"https://data.rennesmetropole.fr/api/explore/v2.1/catalog/datasets/lignes-de-bus-du-reseau-star/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "response = requests.get(url_Z)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_Z, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFZ = pd.read_csv(filename_Z, delimiter=';')\n",
    "    DFZ.to_excel(filename_Z_xlsx, index=False)\n",
    "    print(\"DFZ chargé et exporté en xlsx\")\n",
    "    save_to_db(DFZ, 'dfz')\n",
    "    print(\"Table dfz écrasée dans la base de données.\")\n",
    "else:\n",
    "    print(\"Erreur pour Z\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7469e1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFA chargé\n",
      "DFB chargé\n",
      "DFC chargé\n",
      "DFD chargé\n",
      "DFE chargé\n",
      "DFF chargé\n",
      "DFH chargé\n",
      "DFI chargé\n",
      "DFJ chargé\n",
      "DFK chargé\n",
      "DFL chargé\n",
      "DFM chargé\n",
      "DFN chargé\n",
      "DFO chargé\n",
      "DFP chargé\n",
      "DFQ chargé\n",
      "DFR chargé\n",
      "DFS chargé\n",
      "DFT chargé\n",
      "DFV chargé\n",
      "DFW chargé\n",
      "DFX chargé\n",
      "DFY chargé\n",
      "DFZ chargé\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# A\n",
    "url_A = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/metro_localisation_stations@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_A = \"Métro du réseau STAR  localisation des stations.csv\"\n",
    "response = requests.get(url_A)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_A, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFA = pd.read_csv(filename_A, delimiter=';')\n",
    "    print(\"DFA chargé\")\n",
    "else:\n",
    "    print(\"Erreur pour A\")\n",
    "\n",
    "# B\n",
    "url_B = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/tco-bus-circulation-passages-tr@keolis-rennes/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_B = \"Prochains passages des lignes de bus du réseau STAR en temps réel.csv\"\n",
    "response = requests.get(url_B)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_B, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFB = pd.read_csv(filename_B, delimiter=';')\n",
    "    print(\"DFB chargé\")\n",
    "else:\n",
    "    print(\"Erreur pour B\")\n",
    "\n",
    "# C\n",
    "url_C = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/tco-bus-vehicules-position-tr@keolis-rennes/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_C = \"Position des bus en circulation sur le réseau STAR en temps réel.csv\"\n",
    "response = requests.get(url_C)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_C, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFC = pd.read_csv(filename_C, delimiter=';')\n",
    "    print(\"DFC chargé\")\n",
    "else:\n",
    "    print(\"Erreur pour C\")\n",
    "\n",
    "# D\n",
    "url_D = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/tco-metro-circulation-passages-tr@keolis-rennes/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_D = \"Prochains passages applicables des lignes de métro du réseau STAR.csv\"\n",
    "response = requests.get(url_D)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_D, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFD = pd.read_csv(filename_D, delimiter=';')\n",
    "    print(\"DFD chargé\")\n",
    "else:\n",
    "    print(\"Erreur pour D\")\n",
    "\n",
    "# E\n",
    "url_E = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/vls-stations-etat-tr@keolis-rennes/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_E = \"Etat des stations de vélos en libre-service en temps réel.csv\"\n",
    "response = requests.get(url_E)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_E, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFE = pd.read_csv(filename_E, delimiter=';')\n",
    "    print(\"DFE chargé\")\n",
    "else:\n",
    "    print(\"Erreur pour E\")\n",
    "\n",
    "# F\n",
    "url_F = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/tco-metro-equipements-etat-tr@keolis-rennes/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_F = \"Etat des équipements des stations de métro du réseau STAR en temps réel.csv\"\n",
    "response = requests.get(url_F)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_F, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFF = pd.read_csv(filename_F, delimiter=';')\n",
    "    print(\"DFF chargé\")\n",
    "else:\n",
    "    print(\"Erreur pour F\")\n",
    "\n",
    "# H\n",
    "url_H = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/mkt-frequentation-niveau-freq-max-ligne@keolis-rennes/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_H = \"Niveau de fréquentation maximale par ligne STAR.csv\"\n",
    "response = requests.get(url_H)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_H, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFH = pd.read_csv(filename_H, delimiter=';')\n",
    "    print(\"DFH chargé\")\n",
    "else:\n",
    "    print(\"Erreur pour H\")\n",
    "\n",
    "# I\n",
    "url_I = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/export-api-parking-citedia@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_I = \"Occupation et tarifs des parkings Rennes.csv\"\n",
    "response = requests.get(url_I)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_I, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFI = pd.read_csv(filename_I, delimiter=';')\n",
    "    print(\"DFI chargé\")\n",
    "else:\n",
    "    print(\"Erreur pour I\")\n",
    "\n",
    "# J\n",
    "url_J = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/tco-parcsrelais-star-etat-tr@keolis-rennes/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_J = \"Etat des parcs-relais du réseau STAR en temps réel.csv\"\n",
    "response = requests.get(url_J)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_J, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFJ = pd.read_csv(filename_J, delimiter=';')\n",
    "    print(\"DFJ chargé\")\n",
    "else:\n",
    "    print(\"Erreur pour J\")\n",
    "\n",
    "# K\n",
    "url_K = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/etat-des-lignes-de-metro-du-reseau-star-en-temps-reel@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_K = \"Etat des lignes de métro du réseau STAR en temps réel.csv\"\n",
    "response = requests.get(url_K)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_K, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFK = pd.read_csv(filename_K, delimiter=';')\n",
    "    print(\"DFK chargé\")\n",
    "else:\n",
    "    print(\"Erreur pour K\")\n",
    "\n",
    "# L\n",
    "url_L = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/etat-des-equipements-des-stations-de-metro-du-reseau-star-en-temps-reel@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_L = \"Etat des équipements des stations de métro du réseau STAR en temps réel.csv\"\n",
    "response = requests.get(url_L)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_L, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFL = pd.read_csv(filename_L, delimiter=';')\n",
    "    print(\"DFL chargé\")\n",
    "else:\n",
    "    print(\"Erreur pour L\")\n",
    "\n",
    "# M\n",
    "url_M = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/tco-bus-topologie-parcours-td@keolis-rennes/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_M = \"Parcours des lignes de bus du réseau STAR.csv\"\n",
    "response = requests.get(url_M)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_M, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFM = pd.read_csv(filename_M, delimiter=';')\n",
    "    print(\"DFM chargé\")\n",
    "else:\n",
    "    print(\"Erreur pour M\")\n",
    "\n",
    "# N\n",
    "url_N = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/tco-bus-materiel-vehicules-td@keolis-rennes/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_N = \"Bus utilisés sur le réseau STAR.csv\"\n",
    "response = requests.get(url_N)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_N, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFN = pd.read_csv(filename_N, delimiter=';')\n",
    "    print(\"DFN chargé\")\n",
    "else:\n",
    "    print(\"Erreur pour N\")\n",
    "\n",
    "# O\n",
    "url_O = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/conditions-de-circulation-hivernales@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_O = \"Conditions de circulation hivernales.csv\"\n",
    "response = requests.get(url_O)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_O, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFO = pd.read_csv(filename_O, delimiter=';')\n",
    "    print(\"DFO chargé\")\n",
    "else:\n",
    "    print(\"Erreur pour O\")\n",
    "\n",
    "# P\n",
    "url_P = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/tco-billettique-star-frequentation-agregee-td@keolis-rennes/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_P = \"Données de fréquentation agrégées du réseau STAR au format tableau.csv\"\n",
    "response = requests.get(url_P)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_P, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFP = pd.read_csv(filename_P, delimiter=';')\n",
    "    print(\"DFP chargé\")\n",
    "else:\n",
    "    print(\"Erreur pour P\")\n",
    "\n",
    "# Q\n",
    "url_Q = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/travaux_1_jour@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_Q = \"Travaux de voirie impactant la circulation automobile sur Rennes Métropole pour le jour courant.csv\"\n",
    "response = requests.get(url_Q)\n",
    "if response.status_code == 200:\n",
    "    try:\n",
    "        with open(filename_Q, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        DFQ = pd.read_csv(filename_Q, delimiter=';')\n",
    "        print(\"DFQ chargé\")\n",
    "    except PermissionError:\n",
    "        print(f\"PermissionError: Impossible d'écrire dans {filename_Q}. Fermez le fichier s'il est ouvert dans un autre programme et réessayez.\")\n",
    "        DFQ = None\n",
    "else:\n",
    "    print(\"Erreur pour Q\")\n",
    "\n",
    "# R\n",
    "url_R = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/accidents_corporels@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_R = \"Accidents corporels sur Rennes Métropole.csv\"\n",
    "response = requests.get(url_R)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_R, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFR = pd.read_csv(filename_R, delimiter=';')\n",
    "    print(\"DFR chargé\")\n",
    "else:\n",
    "    print(\"Erreur pour R\")\n",
    "\n",
    "# S\n",
    "url_S = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/tco-billettique-frequentation-detaillee-td@keolis-rennes/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_S = \"Données de fréquentation détaillées du réseau STAR.csv\"\n",
    "response = requests.get(url_S)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_S, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFS = pd.read_csv(filename_S, delimiter=';')\n",
    "    print(\"DFS chargé\")\n",
    "else:\n",
    "    print(\"Erreur pour S\")\n",
    "\n",
    "# T\n",
    "url_T = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/viabilite-hivernale@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_T = \"Viabilité Hivernale (VH) sur Rennes Métropole.csv\"\n",
    "response = requests.get(url_T)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_T, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFT = pd.read_csv(filename_T, delimiter=';')\n",
    "    print(\"DFT chargé\")\n",
    "else:\n",
    "    print(\"Erreur pour T\")\n",
    "\n",
    "# V\n",
    "url_V = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/travaux_6_jours@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_V = \"Travaux de voirie impactant la circulation automobile sur Rennes Métropole pour les 6 prochains jours.csv\"\n",
    "response = requests.get(url_V)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_V, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFV = pd.read_csv(filename_V, delimiter=';')\n",
    "    print(\"DFV chargé\")\n",
    "else:\n",
    "    print(\"Erreur pour V\")\n",
    "\n",
    "# W\n",
    "url_W = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/accidents_corporels_cumul_hors_intersection@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_W = \"Cumul sur 5 ans des accidents corporels hors intersection.csv\"\n",
    "response = requests.get(url_W)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_W, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFW = pd.read_csv(filename_W, delimiter=';')\n",
    "    print(\"DFW chargé\")\n",
    "else:\n",
    "    print(\"Erreur pour W\")\n",
    "\n",
    "# X\n",
    "url_X = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/intervention_securite@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_X = \"Interventions de sécurité de la Direction de la Voirie sur Rennes Métropole.csv\"\n",
    "response = requests.get(url_X)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_X, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFX = pd.read_csv(filename_X, delimiter=';')\n",
    "    print(\"DFX chargé\")\n",
    "else:\n",
    "    print(\"Erreur pour X\")\n",
    "\n",
    "# Y\n",
    "url_Y = \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/troncon_rva_support_fcd_avt_28032023@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_Y = \"Tronçons de voirie du RVA supports aux données de trafic routier temps réel collectées avant le 28032023.csv\"\n",
    "response = requests.get(url_Y)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_Y, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFY = pd.read_csv(filename_Y, delimiter=';')\n",
    "    print(\"DFY chargé\")\n",
    "else:\n",
    "    print(\"Erreur pour Y\")\n",
    "\n",
    "# Z\n",
    "url_Z = \"https://data.rennesmetropole.fr/api/explore/v2.1/catalog/datasets/lignes-de-bus-du-reseau-star/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\"\n",
    "filename_Z = \"Lignes de bus du réseau STAR.csv\"\n",
    "response = requests.get(url_Z)\n",
    "if response.status_code == 200:\n",
    "    with open(filename_Z, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    DFZ = pd.read_csv(filename_Z, delimiter=';')\n",
    "    print(\"DFZ chargé\")\n",
    "else:\n",
    "    print(\"Erreur pour Z\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5b296f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFA chargé\n",
      "✅ Table dfa (ODS) actualisée (30 lignes)\n",
      "DFB chargé\n",
      "✅ Table dfb (ODS) actualisée (4660 lignes)\n",
      "DFC chargé\n",
      "✅ Table dfc (ODS) actualisée (475 lignes)\n",
      "DFD chargé\n",
      "✅ Table dfd (ODS) actualisée (2039 lignes)\n",
      "DFE chargé\n",
      "✅ Table dfe (ODS) actualisée (57 lignes)\n",
      "DFF chargé\n",
      "✅ Table dff (ODS) actualisée (261 lignes)\n",
      "DFH chargé\n",
      "✅ Table dfh (ODS) actualisée (8400 lignes)\n",
      "DFI chargé\n",
      "✅ Table dfi (ODS) actualisée (11 lignes)\n",
      "DFJ chargé\n",
      "✅ Table dfj (ODS) actualisée (8 lignes)\n",
      "DFK chargé\n",
      "✅ Table dfk (ODS) actualisée (2 lignes)\n",
      "DFL chargé\n",
      "✅ Table dfl (ODS) actualisée (261 lignes)\n",
      "DFM chargé\n",
      "✅ Table dfm (ODS) actualisée (579 lignes)\n",
      "DFN chargé\n",
      "✅ Table dfn (ODS) actualisée (631 lignes)\n",
      "FDO chargé\n",
      "❌ Erreur lors de l'insertion de fdo: (psycopg2.errors.UndefinedTable) ERREUR:  la relation « ODS_Projet_etudes.fdo » n'existe pas\n",
      "\n",
      "[SQL: TRUNCATE TABLE \"ODS_Projet_etudes\".\"fdo\" RESTART IDENTITY CASCADE]\n",
      "(Background on this error at: https://sqlalche.me/e/14/f405)\n",
      "DFP chargé\n",
      "✅ Table dfp (ODS) actualisée (27146 lignes)\n",
      "DFQ chargé\n",
      "✅ Table dfq (ODS) actualisée (314 lignes)\n",
      "DFR chargé\n",
      "✅ Table dfr (ODS) actualisée (6261 lignes)\n",
      "DFS chargé\n",
      "✅ Table dfs (ODS) actualisée (38 lignes)\n",
      "DFT chargé\n",
      "✅ Table dft (ODS) actualisée (294 lignes)\n",
      "DFV chargé\n",
      "✅ Table dfv (ODS) actualisée (366 lignes)\n",
      "DFW chargé\n",
      "✅ Table dfw (ODS) actualisée (8790 lignes)\n",
      "DFX chargé\n",
      "✅ Table dfx (ODS) actualisée (2318 lignes)\n",
      "DFY chargé\n",
      "✅ Table dfy (ODS) actualisée (2361 lignes)\n",
      "DFZ chargé\n",
      "✅ Table dfz (ODS) actualisée (169 lignes)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Connexion à la base PostgreSQL\n",
    "engine = create_engine(\"postgresql+psycopg2://postgres:Amyas01062015%40@localhost:5432/olist_ods\")\n",
    "schema = \"ODS_Projet_etudes\"\n",
    "\n",
    "def truncate_and_insert(df, table_name):\n",
    "    with engine.begin() as conn:\n",
    "        # TRUNCATE la table (attention, cascade si FK)\n",
    "        conn.execute(text(f'TRUNCATE TABLE \"{schema}\".\"{table_name}\" RESTART IDENTITY CASCADE'))\n",
    "    # Insert les données\n",
    "    df.to_sql(table_name, engine, schema=schema, if_exists='append', index=False)\n",
    "    print(f\"✅ Table {table_name} (ODS) actualisée ({len(df)} lignes)\")\n",
    "\n",
    "# Liste des jeux de données à charger (lettre, url, nom de fichier, nom de variable)\n",
    "datasets = [\n",
    "    ('dfa', \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/metro_localisation_stations@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\", \"Métro du réseau STAR  localisation des stations.csv\"),\n",
    "    ('dfb', \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/tco-bus-circulation-passages-tr@keolis-rennes/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\", \"Prochains passages des lignes de bus du réseau STAR en temps réel.csv\"),\n",
    "    ('dfc', \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/tco-bus-vehicules-position-tr@keolis-rennes/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\", \"Position des bus en circulation sur le réseau STAR en temps réel.csv\"),\n",
    "    ('dfd', \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/tco-metro-circulation-passages-tr@keolis-rennes/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\", \"Prochains passages applicables des lignes de métro du réseau STAR.csv\"),\n",
    "    ('dfe', \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/vls-stations-etat-tr@keolis-rennes/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\", \"Etat des stations de vélos en libre-service en temps réel.csv\"),\n",
    "    ('dff', \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/tco-metro-equipements-etat-tr@keolis-rennes/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\", \"Etat des équipements des stations de métro du réseau STAR en temps réel.csv\"),\n",
    "    ('dfh', \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/mkt-frequentation-niveau-freq-max-ligne@keolis-rennes/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\", \"Niveau de fréquentation maximale par ligne STAR.csv\"),\n",
    "    ('dfi', \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/export-api-parking-citedia@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\", \"Occupation et tarifs des parkings Rennes.csv\"),\n",
    "    ('dfj', \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/tco-parcsrelais-star-etat-tr@keolis-rennes/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\", \"Etat des parcs-relais du réseau STAR en temps réel.csv\"),\n",
    "    ('dfk', \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/etat-des-lignes-de-metro-du-reseau-star-en-temps-reel@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\", \"Etat des lignes de métro du réseau STAR en temps réel.csv\"),\n",
    "    ('dfl', \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/etat-des-equipements-des-stations-de-metro-du-reseau-star-en-temps-reel@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\", \"Etat des équipements des stations de métro du réseau STAR en temps réel.csv\"),\n",
    "    ('dfm', \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/tco-bus-topologie-parcours-td@keolis-rennes/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\", \"Parcours des lignes de bus du réseau STAR.csv\"),\n",
    "    ('dfn', \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/tco-bus-materiel-vehicules-td@keolis-rennes/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\", \"Bus utilisés sur le réseau STAR.csv\"),\n",
    "    ('fdo', \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/conditions-de-circulation-hivernales@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\", \"Conditions de circulation hivernales.csv\"),\n",
    "    ('dfp', \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/tco-billettique-star-frequentation-agregee-td@keolis-rennes/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\", \"Données de fréquentation agrégées du réseau STAR au format tableau.csv\"),\n",
    "    ('dfq', \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/travaux_1_jour@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\", \"Travaux de voirie impactant la circulation automobile sur Rennes Métropole pour le jour courant.csv\"),\n",
    "    ('dfr', \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/accidents_corporels@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\", \"Accidents corporels sur Rennes Métropole.csv\"),\n",
    "    ('dfs', \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/tco-billettique-frequentation-detaillee-td@keolis-rennes/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\", \"Données de fréquentation détaillées du réseau STAR.csv\"),\n",
    "    ('dft', \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/viabilite-hivernale@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\", \"Viabilité Hivernale (VH) sur Rennes Métropole.csv\"),\n",
    "    ('dfv', \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/travaux_6_jours@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\", \"Travaux de voirie impactant la circulation automobile sur Rennes Métropole pour les 6 prochains jours.csv\"),\n",
    "    ('dfw', \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/accidents_corporels_cumul_hors_intersection@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\", \"Cumul sur 5 ans des accidents corporels hors intersection.csv\"),\n",
    "    ('dfx', \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/intervention_securite@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\", \"Interventions de sécurité de la Direction de la Voirie sur Rennes Métropole.csv\"),\n",
    "    ('dfy', \"https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/troncon_rva_support_fcd_avt_28032023@rennes-metropole/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\", \"Tronçons de voirie du RVA supports aux données de trafic routier temps réel collectées avant le 28032023.csv\"),\n",
    "    ('dfz', \"https://data.rennesmetropole.fr/api/explore/v2.1/catalog/datasets/lignes-de-bus-du-reseau-star/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B\", \"Lignes de bus du réseau STAR.csv\"),\n",
    "]\n",
    "\n",
    "for var, url, filename in datasets:\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        df = pd.read_csv(filename, delimiter=';')\n",
    "        print(f\"{var.upper()} chargé\")\n",
    "        # Truncate puis insert dans la base\n",
    "        try:\n",
    "            truncate_and_insert(df, var)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erreur lors de l'insertion de {var}: {e}\")\n",
    "    else:\n",
    "        print(f\"Erreur pour {var.upper()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb02d5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-01 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-01-02 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-01-03 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-01-04 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-01-05 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-01-06 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-01-07 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-02-24 Vacances d'hiver\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-02-25 Vacances d'hiver\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-02-26 Vacances d'hiver\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-02-27 Vacances d'hiver\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-02-28 Vacances d'hiver\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-02-29 Vacances d'hiver\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-03-01 Vacances d'hiver\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-03-02 Vacances d'hiver\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-03-03 Vacances d'hiver\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-03-04 Vacances d'hiver\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-03-05 Vacances d'hiver\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-03-06 Vacances d'hiver\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-03-07 Vacances d'hiver\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-03-08 Vacances d'hiver\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-03-09 Vacances d'hiver\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-03-10 Vacances d'hiver\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-04-20 Vacances de printemps\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-04-21 Vacances de printemps\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-04-22 Vacances de printemps\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-04-23 Vacances de printemps\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-04-24 Vacances de printemps\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-04-25 Vacances de printemps\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-04-26 Vacances de printemps\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-04-27 Vacances de printemps\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-04-28 Vacances de printemps\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-04-29 Vacances de printemps\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-04-30 Vacances de printemps\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-05-01 Vacances de printemps\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-05-02 Vacances de printemps\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-05-03 Vacances de printemps\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-05-04 Vacances de printemps\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-05-05 Vacances de printemps\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-07-06 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-07-07 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-07-08 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-07-09 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-07-10 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-07-11 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-07-12 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-07-13 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-07-14 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-07-15 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-07-16 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-07-17 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-07-18 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-07-19 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-07-20 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-07-21 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-07-22 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-07-23 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-07-24 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-07-25 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-07-26 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-07-27 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-07-28 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-07-29 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-07-30 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-07-31 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-08-01 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-08-02 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-08-03 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-08-04 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-08-05 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-08-06 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-08-07 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-08-08 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-08-09 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-08-10 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-08-11 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-08-12 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-08-13 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-08-14 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-08-15 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-08-16 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-08-17 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-08-18 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-08-19 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-08-20 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-08-21 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-08-22 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-08-23 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-08-24 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-08-25 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-08-26 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-08-27 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-08-28 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-08-29 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-08-30 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-08-31 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-09-01 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-10-19 Vacances de la Toussaint\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-10-20 Vacances de la Toussaint\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-10-21 Vacances de la Toussaint\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-10-22 Vacances de la Toussaint\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-10-23 Vacances de la Toussaint\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-10-24 Vacances de la Toussaint\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-10-25 Vacances de la Toussaint\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-10-26 Vacances de la Toussaint\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-10-27 Vacances de la Toussaint\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-10-28 Vacances de la Toussaint\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-10-29 Vacances de la Toussaint\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-10-30 Vacances de la Toussaint\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-10-31 Vacances de la Toussaint\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-11-01 Vacances de la Toussaint\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-11-02 Vacances de la Toussaint\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-11-03 Vacances de la Toussaint\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-12-21 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-12-22 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-12-23 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-12-24 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-12-25 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-12-26 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-12-27 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-12-28 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-12-29 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-12-30 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2024-12-31 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-01-01 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-01-02 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-01-03 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-01-04 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-01-05 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-02-08 Vacances d'hiver\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-02-09 Vacances d'hiver\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-02-10 Vacances d'hiver\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-02-11 Vacances d'hiver\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-02-12 Vacances d'hiver\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-02-13 Vacances d'hiver\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-02-14 Vacances d'hiver\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-02-15 Vacances d'hiver\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-02-16 Vacances d'hiver\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-02-17 Vacances d'hiver\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-02-18 Vacances d'hiver\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-02-19 Vacances d'hiver\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-02-20 Vacances d'hiver\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-02-21 Vacances d'hiver\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-02-22 Vacances d'hiver\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-02-23 Vacances d'hiver\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-04-05 Vacances de printemps\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-04-06 Vacances de printemps\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-04-07 Vacances de printemps\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-04-08 Vacances de printemps\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-04-09 Vacances de printemps\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-04-10 Vacances de printemps\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-04-11 Vacances de printemps\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-04-12 Vacances de printemps\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-04-13 Vacances de printemps\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-04-14 Vacances de printemps\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-04-15 Vacances de printemps\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-04-16 Vacances de printemps\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-04-17 Vacances de printemps\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-04-18 Vacances de printemps\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-04-19 Vacances de printemps\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-04-20 Vacances de printemps\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-04-21 Vacances de printemps\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-07-05 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-07-06 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-07-07 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-07-08 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-07-09 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-07-10 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-07-11 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-07-12 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-07-13 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-07-14 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-07-15 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-07-16 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-07-17 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-07-18 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-07-19 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-07-20 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-07-21 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-07-22 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-07-23 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-07-24 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-07-25 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-07-26 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-07-27 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-07-28 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-07-29 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-07-30 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-07-31 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-08-01 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-08-02 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-08-03 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-08-04 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-08-05 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-08-06 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-08-07 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-08-08 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-08-09 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-08-10 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-08-11 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-08-12 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-08-13 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-08-14 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-08-15 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-08-16 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-08-17 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-08-18 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-08-19 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-08-20 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-08-21 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-08-22 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-08-23 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-08-24 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-08-25 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-08-26 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-08-27 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-08-28 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-08-29 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-08-30 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-08-31 Vacances d'été\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-10-18 Vacances de la Toussaint\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-10-19 Vacances de la Toussaint\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-10-20 Vacances de la Toussaint\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-10-21 Vacances de la Toussaint\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-10-22 Vacances de la Toussaint\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-10-23 Vacances de la Toussaint\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-10-24 Vacances de la Toussaint\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-10-25 Vacances de la Toussaint\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-10-26 Vacances de la Toussaint\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-10-27 Vacances de la Toussaint\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-10-28 Vacances de la Toussaint\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-10-29 Vacances de la Toussaint\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-10-30 Vacances de la Toussaint\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-10-31 Vacances de la Toussaint\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-11-01 Vacances de la Toussaint\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-11-02 Vacances de la Toussaint\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-12-20 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-12-21 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-12-22 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-12-23 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-12-24 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-12-25 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-12-26 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-12-27 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-12-28 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-12-29 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-12-30 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n",
      "2025-12-31 Vacances de Noël\n",
      "✅ Table Data_Vacances enregistrée dans la base ods\n"
     ]
    }
   ],
   "source": [
    "###############---------------------------ETL----------------------------###############\n",
    "\n",
    "\n",
    "#-------------------------charement des vacances scolaires dans la tables vacances dans le DWH-------------------------\n",
    "from vacances_scolaires_france import SchoolHolidayDates\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "d = SchoolHolidayDates()\n",
    "\n",
    "# Récupérer toutes les vacances pour la zone B en 2024 et 2025\n",
    "holidays_2024 = d.holidays_for_year_and_zone(2024, 'B')\n",
    "holidays_2025 = d.holidays_for_year_and_zone(2025, 'B')\n",
    "\n",
    "# Fusionner les deux dictionnaires\n",
    "holidays = {**holidays_2024, **holidays_2025}\n",
    "\n",
    "# Filtrer entre janvier 2024 et décembre 2025\n",
    "start = datetime.date(2024, 1, 1)\n",
    "end = datetime.date(2025, 12, 31)\n",
    "holidays_filtered = {date: info for date, info in holidays.items() if start <= date <= end}\n",
    "\n",
    "# Afficher les dates et types de vacances\n",
    "for date, info in sorted(holidays_filtered.items()):\n",
    "    print(date, info['nom_vacances'])\n",
    "\n",
    "    # Création du DataFrame à partir du dictionnaire holidays_filtered\n",
    "    df_vacances = pd.DataFrame([\n",
    "        {\"date\": date, \"nom_vacances\": info[\"nom_vacances\"]}\n",
    "        for date, info in holidays_filtered.items()\n",
    "    ])\n",
    "\n",
    "    # Connexion à la base DWH\n",
    "    engine_ods = create_engine(\"postgresql+psycopg2://postgres:Amyas01062015%40@localhost:5432/olist_ods\")\n",
    "    schema_ods = 'ODS_Projet_etudes'\n",
    "\n",
    "    # Créer le schéma s'il n'existe pas\n",
    "    with engine_ods.connect() as conn:\n",
    "        conn.execute(f'CREATE SCHEMA IF NOT EXISTS \"{schema_ods}\"')\n",
    "\n",
    "    # Sauvegarde dans la table Data_Vacances du schéma ods\n",
    "    df_vacances.to_sql(\"Data_Vacances\", engine_ods, schema=schema_ods, if_exists=\"replace\", index=False)\n",
    "    print(\"✅ Table Data_Vacances enregistrée dans la base ods\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4a481e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Table dfp_frequentations enregistrée dans la base olist_dwh\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------Table dfp_frequentations--------------------------------\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Connexion à la base source (olist_ods)\n",
    "engine_ods = create_engine(\"postgresql+psycopg2://postgres:Amyas01062015%40@localhost:5432/olist_ods\")\n",
    "schema_ods = 'ODS_Projet_etudes'\n",
    "\n",
    "# Charger les tables depuis olist_ods\n",
    "dfp = pd.read_sql_table(\"dfp\", engine_ods, schema=schema_ods)\n",
    "df_vacances = pd.read_sql_table(\"Data_Vacances\", engine_ods, schema=schema_ods)\n",
    "\n",
    "# Conversion des colonnes 'date' au format date\n",
    "dfp['date'] = pd.to_datetime(dfp['date']).dt.date\n",
    "df_vacances['date'] = pd.to_datetime(df_vacances['date']).dt.date\n",
    "# Conversion des colonnes 'Frequentation' au format decimal avec deux chiffres en decimal\n",
    "dfp['Frequentation'] = dfp['Frequentation'].astype(float).round(2)\n",
    "# Jointure gauche pour garder toutes les lignes de dfp\n",
    "df_merged = pd.merge(dfp, df_vacances, on='date', how='left')\n",
    "\n",
    "# Connexion à la base cible (olist_dwh)\n",
    "engine_dwh = create_engine(\"postgresql+psycopg2://postgres:Amyas01062015%40@localhost:5432/olist_dwh\")\n",
    "schema_dwh = 'DWH_Projet_etudes'\n",
    "\n",
    "# Créer le schéma s'il n'existe pas\n",
    "with engine_dwh.connect() as conn:\n",
    "\tconn.execute(f'CREATE SCHEMA IF NOT EXISTS \"{schema_dwh}\"')\n",
    "\n",
    "# Sauvegarde dans la base olist_dwh\n",
    "df_merged.to_sql(\"dfp_frequentations\", engine_dwh, schema=schema_dwh, if_exists=\"replace\", index=False)\n",
    "print(\"✅ Table dfp_frequentations enregistrée dans la base olist_dwh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d609f2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!streamlit run start.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
